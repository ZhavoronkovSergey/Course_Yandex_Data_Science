{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Модель распознавания эмоций из речи.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLTvufw9vg3T"
      },
      "source": [
        "## Модель распознавания эмоций из речи\n",
        " \n",
        " Модель распознавания эмоций из речи с использованием библиотек librosa и sklearn и набора данных RAVDESS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1VM_Ge4vgzF"
      },
      "source": [
        "В этом проекте Python мы будем использовать библиотеки **librosa**, **soundfile** и **sklearn** (среди прочих) для построения модели с использованием MLPClassifier. Это позволит распознавать эмоции из звуковых файлов. \n",
        " \n",
        "\n",
        "**Последовательность выполнения задачи**\n",
        "\n",
        "* загрузим данные\n",
        "* извлекем из них объекты\n",
        "* разделим набор данных на обучающие и тестовые наборы\n",
        "* инициализируем MLP-классификатор и обучим модель \n",
        "* рассчитаем точность нашей модели."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TX_T8I8LvgE3",
        "outputId": "b96e9572-2f42-46ad-c09f-7c7dec32deee"
      },
      "source": [
        "!pip install librosa soundfile"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (0.8.0)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.7/dist-packages (0.10.3.post1)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (2.1.9)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.22.2.post1)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.51.2)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.2.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.3.0)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.0.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.4.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile) (1.14.5)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (54.2.0)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa) (1.15.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (20.9)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile) (2.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pooch>=1.0->librosa) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KNK1Br3vbyj"
      },
      "source": [
        "import librosa\n",
        "import soundfile\n",
        "import os, glob, pickle\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjSls9dwtIz8",
        "outputId": "a7c4700a-5969-4890-8d79-e9c6e204840a"
      },
      "source": [
        "#Connect your Drive with Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IW4sKRKZtQEb",
        "outputId": "a52c0c72-b531-4471-cf0c-6a365c38018a"
      },
      "source": [
        "#Unzip the file contents\n",
        "!unzip '/content/drive/MyDrive/speech-emotion-recognition-ravdess-data.zip'"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/speech-emotion-recognition-ravdess-data.zip\n",
            "replace Actor_01/03-01-01-01-01-01-01.wav? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ9CevWQ2dSe"
      },
      "source": [
        "Определение функции extract_feature для извлечения функций mfcc, chroma и mel из звукового файла. Эта функция принимает 4 параметра - имя файла и три логических параметра для трех функций:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4454X_nr3BKX"
      },
      "source": [
        "* mfcc:  представляет собой кратковременный спектр мощности звука (переходы в звуковых тонах)\n",
        "* chroma: Относится к 12 различным классам высоты тона\n",
        "* mel: Частота Спектрограммы Mel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX4JUgQh4Ap1"
      },
      "source": [
        "Открываем звуковой файл с помощью soundfile.SoundFile используя with-as, далее файл автоматически закрывается, как только мы закончим. прочтем его и назовем X. Кроме того, получим частоту дискретизации. Если chroma истинна, получим преобразование Фурье X.\n",
        "\n",
        "Пусть результатом будет пустой массив numpy. Теперь для каждого параметра функции, сделаем вызов соответствующей функции из librosa.feature (например - librosa.feature.mfcc для mfcc) и получим среднее значение. Вызовите функцию hstack() из numpy значение функции и сохраним в result. hstack() соединят массивы по горизонтали. Затем вернем результ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_63D_I4UtP9S"
      },
      "source": [
        "# определение функции extract_feature для извлечения функций mfcc, chroma и mel из звукового файла\n",
        "def extract_feature(file_name, mfcc, chroma, mel):\n",
        "  with soundfile.SoundFile(file_name) as sound_file:\n",
        "    X = sound_file.read(dtype=\"float32\")\n",
        "    sample_rate=sound_file.samplerate\n",
        "    if chroma:\n",
        "      stft=np.abs(librosa.stft(X))\n",
        "    result=np.array([])\n",
        "    if mfcc:\n",
        "      mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
        "      result=np.hstack((result, mfccs))\n",
        "    if chroma:\n",
        "      chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
        "      result=np.hstack((result, chroma))\n",
        "    if mel:\n",
        "      mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
        "      result=np.hstack((result, mel))\n",
        "  return result"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnex7KaztP6K"
      },
      "source": [
        "# определим словарь эмоций \n",
        "emotions = {\n",
        "    '01':'neutral',\n",
        "    '02':'calm',\n",
        "    '03':'happy',\n",
        "    '04':'sad',\n",
        "    '05':'angry',\n",
        "    '06':'fearful',\n",
        "    '07':'disgust',\n",
        "    '08':'surprised'\n",
        "}\n",
        "\n",
        "# рассматриваемые эмоции\n",
        "observed_emotions = ['calm', 'happy', 'fearful', 'disgust']"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFlh3VmB_Bo_"
      },
      "source": [
        "Теперь загрузим данные с помощью функции load_data() – она принимает в качестве параметра относительный размер тестового набора. x и y-пустые списки;\n",
        "Будем использовать функцию glob() из модуля glob, чтобы получить все пути к звуковым файлам в нашем наборе данных.\n",
        "Получите базовое имя файла, эмоцию, разделив имя ‘-’ и извлекая третье значение"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxUKvU06_yTX"
      },
      "source": [
        "Используя наш словарь эмоций, число соответствует эмоции, и наша функция проверяет, есть ли эта эмоция в нашем списке observed_emotions; если нет, она переходит к следующему файлу. Он вызывает extract_feature и сохраняет то, что возвращается в \"feature\". Затем он добавляет функцию к x, а эмоцию-к y. Итак, список x содержит черты, а y-эмоции. Мы вызываем функцию train_test_split с этими данными, размером теста и случайным значением состояния и возвращаем его."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_0D3vDrtP3D"
      },
      "source": [
        "# загрузим данные и извлеките функции для каждого звукового файла\n",
        "def load_data(test_size = 0.2):\n",
        "  x, y = [], []\n",
        "  for folder in glob.glob('/content/Actor_*'):\n",
        "    print(folder)\n",
        "    for file in glob.glob(folder + '/*.wav'):\n",
        "      file_name = os.path.basename(file)\n",
        "      emotion = emotions[file_name.split('-')[2]]\n",
        "      if emotion not in observed_emotions:\n",
        "        continue\n",
        "      feature = extract_feature(file, mfcc = True, chroma = True, mel = True)\n",
        "      x.append(feature)\n",
        "      y.append(emotion)\n",
        "  return train_test_split(np.array(x), y, test_size = test_size, random_state = 9)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMkhaYNrtP0Y",
        "outputId": "d0ad6a72-bb72-4c99-b154-3c8a88710746"
      },
      "source": [
        "x_train,x_test,y_train,y_test=load_data(test_size=0.2)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Actor_17\n",
            "/content/Actor_03\n",
            "/content/Actor_08\n",
            "/content/Actor_23\n",
            "/content/Actor_19\n",
            "/content/Actor_09\n",
            "/content/Actor_10\n",
            "/content/Actor_14\n",
            "/content/Actor_05\n",
            "/content/Actor_21\n",
            "/content/Actor_01\n",
            "/content/Actor_15\n",
            "/content/Actor_11\n",
            "/content/Actor_18\n",
            "/content/Actor_13\n",
            "/content/Actor_02\n",
            "/content/Actor_22\n",
            "/content/Actor_04\n",
            "/content/Actor_07\n",
            "/content/Actor_24\n",
            "/content/Actor_20\n",
            "/content/Actor_16\n",
            "/content/Actor_06\n",
            "/content/Actor_12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03_TrRsCtPxi",
        "outputId": "1de72309-1bdf-40eb-d143-8adb3f9b9e09"
      },
      "source": [
        "print((x_train.shape[0], x_test.shape[0]))\n",
        "print(f'Features extracted: {x_train.shape[1]}')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(614, 154)\n",
            "Features extracted: 180\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHK0RMVfD7xC"
      },
      "source": [
        "Теперь давайте инициализируем MLPClassifier. Это многослойный персептронный классификатор, он оптимизирует функцию логарифмических потерь с помощью LBFGS или стохастического градиентного спуска. В отличие от SVM или наивного Байеса, MLPClassifier имеет внутреннюю нейронную сеть для целей классификации. Это модель прямой ANN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87TBrRT2tPuc"
      },
      "source": [
        "# инициализируем Multi Layer Perceptron Classifier\n",
        "model = MLPClassifier(alpha = 0.01, batch_size = 256, epsilon = 1e-08, hidden_layer_sizes = (300,), learning_rate = 'adaptive', max_iter = 500)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0cRXkp2tPrr",
        "outputId": "8e1a9c5e-2278-47a1-e22c-240547682036"
      },
      "source": [
        "model.fit(x_train, y_train)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.01, batch_size=256, beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(300,), learning_rate='adaptive',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=500,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6th-6r1tPoy"
      },
      "source": [
        "y_pred = model.predict(x_test)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGjVcJqbtPmF",
        "outputId": "d3453883-8db3-4bf0-9f84-b78a878c7306"
      },
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 76.62%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9Sy-_XDII-A"
      },
      "source": [
        "## Вывод\n",
        "  В этом мини-проекте с помощью MLPClassifier и librosa в представлении пространственных объектов, трансформировании и кодировании последовательностей мне удается достичь точности >75% на тестовом наборе данных RAVDESS."
      ]
    }
  ]
}